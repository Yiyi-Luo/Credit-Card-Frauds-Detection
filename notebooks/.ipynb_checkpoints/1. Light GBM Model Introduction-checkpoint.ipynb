{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae6d3018-4e25-4fe8-b046-70fd47498388",
   "metadata": {},
   "source": [
    "**Light GBM Model Introduction: Table of Contents** \n",
    "\n",
    "---\n",
    "\n",
    "- [1. Gradient Boosting Framework and Basic Concepts](#1-gradient-boosting-framework-and-basic-concepts)\n",
    "  - [1.1. Initial Prediction](#11-initial-prediction)\n",
    "  - [1.2. Residuals Calculation](#12-residuals-calculation)\n",
    "  - [1.3. Splitting Nodes and Features](#13-splitting-nodes-and-features)\n",
    "    - [1.3.1. Features Drive Splits](#131-features-drive-splits)\n",
    "    - [1.3.2. Node Splitting Criteria](#132-node-splitting-criteria)\n",
    "    - [1.3.3. Goal in Splitting Nodes](#133-goal-in-splitting-nodes)\n",
    "  - [1.4. Basic Concepts](#14-loss-function)\n",
    "    - [1.4.1. Residuals](#141-residuals)\n",
    "    - [1.4.2. Loss Function](#142-loss-function)\n",
    "    - [1.4.3. Gradient and Hessian](#143-gradient-and-hessian)\n",
    "  - [1.5. What Happens After the Model Picks the Best Split Point: Iterative Refinement and Combining Trees](#16-what-happens-after-the-model-picks-the-best-split-point-iterative-refinement-and-combining-trees)\n",
    "- [2. Key Features in Light GBM](#2-key-features-in-light-gbm)\n",
    "  - [2.1. Histogram-Based Decision Tree Learning](#21-histogram-based-decision-tree-learning)\n",
    "  - [2.2. Exclusive Features Bundling (EFB)](#22-exclusive-features-bundling-efb)\n",
    "  - [2.3. Built-in Regularization in LightGBM](#23-built-in-regularization-in-lightgbm)\n",
    "  - [2.4. Built-in Function Handling Missing Values in LightGBM](#24-built-in-function-handling-missing-values-in-lightgbm)\n",
    "- [3. Data Preprocessing Goals and Tips](#3-data-preprocessing-goals-and-tips)\n",
    "  - [3.1. Data Cleaning and Normalization](#31-data-cleaning-and-normalization)\n",
    "  - [3.2. Handle Missing Values Appropriately](#32-handle-missing-values-appropriately)\n",
    "  - [3.3. Encode Categorical Variables Effectively](#33-encode-categorical-variables-effectively)\n",
    "  - [3.4. Handle Class Imbalance](#34-handle-class-imbalance)\n",
    "  - [3.5. Outlier Detection and Handling](#35-outlier-detection-and-handling)\n",
    "  - [3.6. Address Multicollinearity](#36-address-multicollinearity)\n",
    "  - [3.7. Feature Engineering:](#37-feature-engineering)\n",
    "    - [3.7.1. Feature Importance and Selection](#371-Feature-Importance-and-selection)\n",
    "    - [3.7.2. Dimensionality Reduction](#372-Dimensionality-Reduction)\n",
    "    - [3.7.3. Feature Creation](#373-Feature-Creation)\n",
    "    - [3.7.4. Handle Imbalanced Feature Distribution](#374-Handle-Imbalanced-feature-distribution)\n",
    "  - [3.12. Balance Data Across Subsets (If Segmented)](#312-balance-data-across-subsets-if-segmented)\n",
    "  - [3.13. Monitor and Validate Model Performance](#313-monitor-and-validate-model-performance)\n",
    "- [4. Model Development](#4-model-development)\n",
    "  - [4.1. Model Selection](#41-model-selection)\n",
    "  - [4.2. Cross-Validation and Hyperparameter Tuning](#42-cross-validation-and-hyperparameter-tuning)\n",
    "- [5. Model Evaluation](#5-model-evaluation)\n",
    "  - [5.1. Evaluation Metrics](#51-evaluation-metrics)\n",
    "- [6. Conclusion](#6-conclusion)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "593b745c-9504-4535-b1b9-6f7bc71a2d86",
   "metadata": {},
   "source": [
    "# 1. Gradient Boosting Framework and Basic Concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c786dec7-2786-4e91-a077-93683f1ae596",
   "metadata": {},
   "source": [
    "## 1.1. Initial Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bec522f-68da-4b08-ae68-97911dc31d39",
   "metadata": {},
   "source": [
    "Before building any trees, the model starts with an initial prediction, often a constant value (e.g., the mean of the target variable in regression or the log-odds in classification). This initial model might predict all instances as the majority class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9c92de-56e5-4558-85af-3c15b7c0883c",
   "metadata": {},
   "source": [
    "**Dataset**:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db57775b-edf3-4218-a640-95a2d33e5393",
   "metadata": {},
   "source": [
    "| Fruit ID | Size   | Color  | Class (Fruit Type) |\r\n",
    "|----------|--------|--------|-------------------|\r\n",
    "| 1        | Small  | Red    | Apple (0)         |\r\n",
    "| 2        | Medium | Red    | Apple (0)         |\r\n",
    "| 3        | Large  | Green  | Apple (0)         |\r\n",
    "| 4        | Medium | Yellow | Banana (1)        |\r\n",
    "| 5        | Large  | Yellow | Banana (1)        |\r\n",
    "| 6        | Small  | Green  | Apple (0)         |\r\n",
    "| 7        | Small  | Yellow | Banana (1)        |\r\n",
    "| 8        | Medium | Green  | Apple (0)         |ple (0)\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e34d266b-e230-459d-ae6f-01d6d5a71de6",
   "metadata": {},
   "source": [
    "**Encoded Dataset**:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac17d44-b403-47a7-b8f8-f3795afde84b",
   "metadata": {},
   "source": [
    "| Fruit ID | Size | Color | Class (Fruit Type) |\r\n",
    "|----------|------|-------|-------------------|\r\n",
    "| 1        | 1    | 1     | 0                 |\r\n",
    "| 2        | 2    | 1     | 0                 |\r\n",
    "| 3        | 3    | 2     | 0                 |\r\n",
    "| 4        | 2    | 3     | 1                 |\r\n",
    "| 5        | 3    | 3     | 1                 |\r\n",
    "| 6        | 1    | 2     | 0                 |\r\n",
    "| 7        | 1    | 3     | 1                 |\r\n",
    "| 8        | 2    | 2     | 0                 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f074c1d7-3227-4940-81d9-76aa90a4fe8e",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741ed189-e106-4e6c-82df-efb51b01b921",
   "metadata": {},
   "source": [
    "## 1.2. Residuals Calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da28e242-46d4-4992-952d-baec2d669f05",
   "metadata": {},
   "source": [
    "Residuals represent the errors between the actual values and the model's predictions. In gradient boosting, these residuals are treated as the target for the next tree. The subsequent tree learns to predict these residuals using the features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e682ba-f4a0-4fc0-8903-d41dc62e964e",
   "metadata": {},
   "source": [
    "| Fruit ID | Actual Class (yiy_iyi) | Predicted Probability | Residual           |\r\n",
    "|----------|------------------------|----------------------|-------------------|\r\n",
    "| 1        | 0                      | 0.375                | 0 - 0.375 = -0.3750 |\r\n",
    "| 2        | 0                      | 0.375                | 0 - 0.375 = -0.3750 |\r\n",
    "| 3        | 0                      | 0.375                | 0 - 0.375 = -0.3750 |\r\n",
    "| 4        | 1                      | 0.375                | 1 - 0.375 = 0.6250  |\r\n",
    "| 5        | 1                      | 0.375                | 1 - 0.375 = 0.6250  |\r\n",
    "| 6        | 0                      | 0.375                | 0 - 0.375 = -0.3750 |\r\n",
    "| 7        | 1                      | 0.375                | 1 - 0.375 = 0.6250  |\r\n",
    "| 8        | 0                      | 0.375                | 0 - 0.375 = -0.3750 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f76d32-488d-4246-ba64-f358ea05ff8a",
   "metadata": {},
   "source": [
    "\r\n",
    "### Interpretation of Residuals\r\n",
    "\r\n",
    "#### Residual of -0.375\r\n",
    "- **Occurs when**: The actual class is Apple (0)\r\n",
    "- **Meaning**: The model overestimated the probability of being a Banana\r\n",
    "- **Interpretation**: The model predicted a 37.5% chance of being a Banana, but it's actually an Apple\r\n",
    "- **Action needed**: Decrease the predicted probability for this instance\r\n",
    "\r\n",
    "#### Residual of 0.625\r\n",
    "- **Occurs when**: The actual class is Banana (1)\r\n",
    "- **Meaning**: The model underestimated the probability of being a Banana\r\n",
    "- **Interpretation**: The model predicted a 37.5% chance of being a Banana, and it is indeed a Banana\r\n",
    "- **Action needed**: Increase the predicted probability for this instance\r\n",
    "\r\n",
    "### Importance of Residuals\r\n",
    "\r\n",
    "1. **Purpose**: Indicate errors in the model's predictions\r\n",
    "2. **Guide Corrections**:\r\n",
    "   - Negative Residuals (-0.375): Model needs to reduce predicted probability\r\n",
    "   - Positive Residuals (0.625): Model needs to increase predicted probability\r\n",
    "\r\n",
    "### Influence on the Next Tree\r\n",
    "\r\n",
    "- Residuals become target values for training the next tree\r\n",
    "- The tree learns patterns in features (Size, Color) to correct predictions\r\n",
    "\r\n",
    "### Analogy for Understanding\r\n",
    "\r\n",
    "Imagine a guessing game:\r\n",
    "1. **Task**: Guess a number between 0 and 1 representing the probability of a fruit being a Banana\r\n",
    "2. **Initial Guess**: 0.375 for all fruits\r\n",
    "3. **Feedback**:\r\n",
    "   - If Apple (0): Guess of 0.375 is too high → Negative feedback of -0.375\r\n",
    "   - If Banana (1): Guess of 0.375 is too low → Positive feedback of 0.625\r\n",
    "4. **Adjustments**:\r\n",
    "   - For Apples: Decrease your next guess\r\n",
    "   - For Bananas: Increase your next guess\r\n",
    "\r\n",
    "This process helps refine predictions in subsequent iterations.\r\n",
    "\r\n",
    "### Next Steps\r\n",
    "\r\n",
    "- Use features (Size, Color) to adjust predictions\r\n",
    "- Trees learn patterns to increase or decrease probabilities appropriately\r\n",
    "- This iterative process continues, with each tree focusing on correcting the errors of the previous trees"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6660b005-684a-4711-b035-4dd18486a194",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b65343-1d17-42f1-b333-27dc4610a2f0",
   "metadata": {},
   "source": [
    "## 1.3. Splitting Nodes and Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba00b67-c279-4e1c-9e3f-e780539ecfe8",
   "metadata": {},
   "source": [
    "### 1.3.1. Features Drive Splits\n",
    "- **Splitting Based on Features**: The tree uses features to split the data into groups with similar residuals.\n",
    "- **Leaf Values Adjust Predictions**: Each leaf assigns a value that adjusts the model's predictions in the direction needed to minimize the loss.\n",
    "- **Learning from Patterns**: For example, the model might learn that yellow fruits are more likely bananas, while red and green fruits are more likely apples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e24872ea-af1c-49a1-b188-23481f51238f",
   "metadata": {},
   "source": [
    "### 1.3.2. Node Splitting Criteria\n",
    "\r\n",
    "- **Evaluate All Possible Splits**: For each feature and potential split point (at bin boundaries\n",
    "\r\n",
    "\r\n",
    "- **Calculate the Gain**: Gain is the reduction in the loss function that would result from the split. It involves computing statistics like sums of gradients and sums of squared gradients (via Hessianplit\r\n",
    "\r\n",
    "- **Maximize Gain**: Choose the split that results in the largest reduction of the loss fuction.\r\n",
    "\r\n",
    "- **Effect on Residuals**: The ideal split separates residuals into groups where they are as similar as possible, often corresponding to minimizing the variance within each child node."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ede2a2-9e4f-475a-bbe1-458375264f81",
   "metadata": {},
   "source": [
    "### 1.3.3. Goal in Splitting Nodes\n",
    "- **Maximizing Loss Reduction**: The primary goal when splitting nodes is to find splits that maximize the reduction in the loss function.\n",
    "- **Low Variance in Residuals**: This often corresponds to creating child nodes where residuals are as similar as possible (i.e., low variance). Low Variance in Residuals: When residuals within a node have low variance, the model can make a consistent adjustment for all instances in that node. This adjustment moves the predictions closer to the actual values. High Variance in Residuals: When the variance is high, it's harder to find a single adjustment that fits all instances in the node. The model's corrections may be less effective.\n",
    "- **Selecting the Best Split**: The ideal split separates residuals into groups where they are as homogeneous as possible, facilitating effective adjustments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb26b2b5-df9d-4761-bf6b-5676dcece2a4",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d26b30cc-b784-4b5f-bbf7-60955e67c059",
   "metadata": {},
   "source": [
    "## 1.4. Basic Concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeacc4a3-506f-4072-b69f-6ffebf6f7c57",
   "metadata": {},
   "source": [
    "### 1.4.1. Residuals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae8c6bb-9d97-4f50-ab60-22e8a4eb1786",
   "metadata": {},
   "source": [
    "### 1.4.2. Loss Function\n",
    "- **Purpose**: The loss function measures the discrepancy between the predicted outputs and the actual outputs. It not only quantifies the prediction error but is also sensitive to the confidence of predictions.\n",
    "- **Sensitivity to Confidence**: In classification tasks, the loss function penalizes not just incorrect predictions but also how confident the model is in those incorrect predictions.\n",
    "- **Example**: Mean Squared Error (MSE) in regression tasks calculates the degree of error between predicted outputs and actual outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3327f72e-fa56-4a0c-b3bb-45955ebf64a3",
   "metadata": {},
   "source": [
    "### 1.4.3. Gradient and Hessian\n",
    "- **Gradients (First-Order Derivatives)**: The residuals are the gradients of the loss function with respect to the predictions.\n",
    "- **Hessians (Second-Order Derivatives)**: The algorithm uses both gradients and second-order derivatives (Hessians) to find the best splits. This second-order information allows for more accurate estimation of loss reduction and better split decisions.\n",
    "- **Benefit**: This allows for more accurate estimation of the loss reduction and better split decisions, improving the model's performance.\n",
    "- **Gain Calculation**: A higher gain indicates a better split. The gain is calculated using the sums of gradients and Hessians in potential splits."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f2dda4-d6fa-4543-a89a-bf3d5bcecd6b",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99da93d6-22e9-40dc-a1a2-b4505c524417",
   "metadata": {},
   "source": [
    "## 1.5. What Happens After the Model Picks the Best Split Point: Iterative Refinement and Combining Trees\r\n",
    "\r\n",
    "- **Correcting Residuals**: Subsequent trees continue to use features to correct residuals from the previous iterations.\r\n",
    "- **Ensemble Model**: Each tree contributes to the final prediction, and the combination of all trees forms the ensemble model.\r\n",
    "- **Continuous Improvement**: Each tree built in the sequence focuses on correcting the errors of the previous ensemble of trees. This iterative process continues until the model reaches a desired level of accuracy.\r\n",
    "\r\n",
    "### Recursive Tree Growth in LightGBM\r\n",
    "\r\n",
    "After LightGBM selects the best split point based on the histogram bins, it doesn't stop there. The model continues to grow the tree recursively by:\r\n",
    "\r\n",
    "- **Creating Child Nodes:** The data is partitioned into left and right child nodes based on the best split.\r\n",
    "- **Repeating the Splitting Process:** For each child node, LightGBM repeats the process of finding the best split using the same histogram-based approach.\r\n",
    "- **Using Bins at Each Level:** The binning process is applied within each child node independently. Even though the initial bins are based on the entire dataset, the distribution of data within each node may differ, affecting how the bins are utilized in subsequent splits.\r\n",
    "\r\n",
    "### Key Points\r\n",
    "\r\n",
    "- **Refinement Through Multiple Splits:** Even though the initial split is based on bins, subsequent splits can further refine the separation of data, effectively \"diving into\" the bins to improve accuracy.\r\n",
    "- **Reusing Binning Strategy:** The histogram bins are a tool to speed up computation but don't limit the tree's depth or the model's ability to capture complex patterns.\r\n",
    "- **Splitting on the Same Feature Multiple Times:** LightGBM can split on the same feature multiple times at different levels of the tree, allowing for nuanced decision boundaries.\r\n",
    "\r\n",
    "### Example\r\n",
    "\r\n",
    "- Suppose the first split is on **Color ≤ 2.5**, separating apples and bananas.\r\n",
    "- In the left child node (apples), the model may then find the best split on **Size ≤ 1.5**.\r\n",
    "- This process continues recursively until stopping criteria are met (e.g., maximum depth, minimum number of samples in a node)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea2f5ef-13f9-4643-bebc-c66acf561a1d",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed6408f-d988-49f6-9208-ef6bcb2b9736",
   "metadata": {},
   "source": [
    "# 2. Key Features in Light GBM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3620deaf-4770-40de-9b30-acc104dbd7e2",
   "metadata": {},
   "source": [
    "## 2.1. Histogram-Based Decision Tree Learning\n",
    "- **Purpose**: Instead of evaluating every possible split point, LightGBM discretizes continuous features into a fixed number of bins (histograms). This reduces the number of potential split points and speeds up computation.\n",
    "- **How It Works**:\n",
    "#### a. Bin Construction\n",
    "- **For Each Feature**:\n",
    "  - The feature values are sorted.\n",
    "  - The range of values is divided into **k** bins (e.g., 255 bins by default).\n",
    "  - Each feature value is assigned to a bin based on its value.\n",
    "- **Result**: Each feature is represented by a histogram of bin counts.\n",
    "#### b. Statistics Collection\n",
    "- **For Each Bin**:\n",
    "  - Accumulate the sum of gradients and Hessians (second-order gradients) for instances falling into that bin.\n",
    "  - These statistics are used to evaluate the potential splits.\n",
    "#### c. Split Finding\n",
    "- **For Each Feature**:\n",
    "  - Use the histogram to evaluate possible splits at bin boundaries.\n",
    "  - Compute the gain (reduction in the loss function) for splits between bins.\n",
    "- **Across All Features**:\n",
    "  - Compare the best splits from each feature.\n",
    "  - Select the split with the highest gain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f836f65d-d3d8-4dcd-9502-af49169fff95",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b22060-0642-4044-bc57-190f02f71deb",
   "metadata": {},
   "source": [
    "## 2.2. Exclusive Feature Bundling (EFB)\r\n",
    "\r\n",
    "- **Key Idea**: Reduce the number of features by combining mutually exclusive features without losing information.\r\n",
    "\r\n",
    "### What Are Mutually Exclusive Features?\r\n",
    "\r\n",
    "- **Definition**: Features that are sparse and rarely non-zero at the same time.\r\n",
    "- **Common in High-Dimensional Data**: For example, one-hot encoded categorical features.\r\n",
    "\r\n",
    "### How EFB Works\r\n",
    "\r\n",
    "#### a. Feature Analysis\r\n",
    "\r\n",
    "- Identify features that are mutually exclusive.\r\n",
    "\r\n",
    "#### b. Feature Bundling\r\n",
    "\r\n",
    "- Combine these features into a single feature (bundle).\r\n",
    "- The combined feature takes on different values to represent the original features.\r\n",
    "\r\n",
    "#### c. Split Finding on Bundles\r\n",
    "\r\n",
    "- When evaluating splits, LightGBM can decode which original feature the value corresponds to.\r\n",
    "\r\n",
    "### Benefits\r\n",
    "\r\n",
    "- **Reduces Number of Features**: Fewer features mean fewer histograms and less computation.\r\n",
    "- **Preserves Information**: No significant loss of information because original features are mutually exclusive.tures are mutually exclusive."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "233222ea-be39-4032-bcbe-b6158279b20f",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b9b91b-28c3-4d72-93b1-6a6df51fd00a",
   "metadata": {},
   "source": [
    "## 2.3. Built-in Regularization in LightGBM\r\n",
    "\r\n",
    "### How Many Built-in Functions Like L1 and L2 Does LightGBM Have?\r\n",
    "\r\n",
    "LightGBM includes several built-in features and regularization techniques that help prevent overfitting and enhance model performance:\r\n",
    "\r\n",
    "#### Regularization Parameters:\r\n",
    "\r\n",
    "- **lambda_l1:** L1 regularization term on weights. It can help induce sparsity in the model.\r\n",
    "- **lambda_l2:** L2 regularization term on weights. It can prevent overfitting by penalizing large weights.\r\n",
    "- **min_gain_to_split:** The minimum loss reduction required to make a split. It acts as a regularization parameter to prevent creating insignificant splits.\r\n",
    "\r\n",
    "#### Default Settings and Automatic Handling:\r\n",
    "\r\n",
    "- **Optimized Defaults:** LightGBM comes with default parameter values that are well-tuned for many problems.\r\n",
    "- **Automatic Feature Handling:** It handles categorical variables, missing values, and overfitting prevention out of the box.\r\n",
    "\r\n",
    "#### Other Built-in Features:\r\n",
    "\r\n",
    "- **Early Stopping:** Stops training when the model's performance on a validation set stops improving.\r\n",
    "- **Feature Fraction (feature_fraction):** Controls the fraction of features to consider when looking for the best split.\r\n",
    "- **Bagging Fraction (bagging_fraction) and Bagging Frequency (bagging_freq):** Used for subsampling the data to prevent overfitting.\r\n",
    "\r\n",
    "### Why One Could Get Good Results Without Much Tuning:\r\n",
    "\r\n",
    "- **Robust Defaults:** The default parameters in LightGBM are designed to work well across a variety of datasets.\r\n",
    "- **Built-in Regularization:** Even without explicit tuning, LightGBM applies regularization techniques that help prevent overfitting.\r\n",
    "- **Efficient Algorithms:** LightGBM's algorithms are optimized for speed and performance, which can lead to good results quickly.\r\n",
    "\r\n",
    "### Takeaway:\r\n",
    "\r\n",
    "- **Out-of-the-Box Performance:** It's possible to achieve strong results with LightGBM without extensive parameter tuning due to its thoughtful defaults and built-in functionalities.\r\n",
    "- **Customization for Improvement:** While defaults work well, further tuning of parameters like regularization terms, learning rate, and tree complexity can lead to even better performance tailored to your specific dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f90bda-583b-45e2-a018-d11accf5e860",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f42ec55-5463-4656-a194-58049972ebb4",
   "metadata": {},
   "source": [
    "## 2.4. Built-in Function Handling Missing Values in LightGBM\r\n",
    "\r\n",
    "### 2.4.1. Filling NaNs with Negative Values: Benefits and Drawbacks\r\n",
    "\r\n",
    "#### Potential Benefits:\r\n",
    "- **Separate Bin for Missing Values:** Negative values will form a separate bin, potentially allowing the model to handle missing data distinctly.\r\n",
    "\r\n",
    "#### Potential Drawbacks:\r\n",
    "- **Introducing Bias:** Artificially assigning a value may introduce bias if negative values don't naturally occur in the data.\r\n",
    "- **Misrepresenting Data:** Negative placeholders may mislead the model into interpreting missingness as a meaningful negative value.\r\n",
    "- **Loss of Missingness Information:** The fact that a value is missing is itself informative and might be better handled explicitly.\r\n",
    "\r\n",
    "### 2.4.2. LightGBM's Native Handling of Missing Values\r\n",
    "\r\n",
    "- **Automatic Handling:** LightGBM can handle missing values natively without the need to impute or fill them.\r\n",
    "- **Best Direction for Missing Values:** During training, when considering a split, LightGBM decides whether missing values should go to the left or right child node based on which option minimizes the loss.\r\n",
    "- **Efficiency in Splitting:** This method allows the model to learn patterns associated with missingness directly, potentially improving performance.\r\n",
    "- **Preserves Missingness Information:** Missing values may carry information (e.g., missing income data might be indicative in fraud detection), and LightGBM leverages this.\r\n",
    "- **Simplifies Preprocessing:** Eliminates the need for manual imputation or complex preprocessing steps.\r\n",
    "\r\n",
    "### 2.4.3. Implementation Details\r\n",
    "\r\n",
    "- **Sparse Features Handling:** LightGBM is efficient with sparse data and treats zeros and missing values appropriately.\r\n",
    "- **Flexibility:** The model can capture interactions between missingness and other features.\r\n",
    "\r\n",
    "### 2.4.4. Recommendations\r\n",
    "\r\n",
    "- **Use Default Missing Value Handling:** Rely on LightGBM's native capability to manage missing values effectively.\r\n",
    "- **Avoid Artificial Imputation:** If you choose to impute missing values, ensure that the method aligns with the data's nature and doesn't introduce unintended artifacts.\r\n",
    "- **Monitor Performance:** While LightGBM handles missing values well, it's still good practice to monitor model performance and consider additional preprocessing if necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a49c4684-4994-4876-918b-465cc3faef61",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd0ec0c-e91c-49f6-99de-373d181211ba",
   "metadata": {},
   "source": [
    "# 3. Data Preprocessing Goals and Tips"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c627d278-a21c-49f4-9016-06b07a829f92",
   "metadata": {},
   "source": [
    "## 3.1. Data Cleaning and Normalization\r\n",
    "\r\n",
    "### 3.1.1. Data Cleaning\r\n",
    "\r\n",
    "Ensure the dataset is accurate and consistent:\r\n",
    "\r\n",
    "- **Remove Duplicates**: Identify and eliminate duplicate records that may bias the model.\r\n",
    "- **Correct Errors**: Fix typos, incorrect entries, or inconsistencies in categorical labels.\r\n",
    "- **Standardize Formats**: Ensure consistency in data formats, units, and scales.\r\n",
    "- **Tip**: Consistent and accurate data is foundational for any machine learning model's success.\r\n",
    "\r\n",
    "### 3.1.2. Normalize or Scale Features (When Appropriate)\r\n",
    "\r\n",
    "While tree-based models like LightGBM don't require feature scaling, in some cases, normalization can help:\r\n",
    "\r\n",
    "- **When to Scale**: If you have features on vastly different scales and suspect it may impact the model's performance.\r\n",
    "- **Scaling Methods**:\r\n",
    "  - Standardization: Subtract mean and divide by standard deviation.\r\n",
    "  - Normalization: Scale features to a [0, 1] range.\r\n",
    "- **Tip**: Generally, scaling is not necessary for LightGBM, but it won't harm the model if applied thoughtfully."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2beefe8c-6de6-42c2-a7a7-96efa2f9de12",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "323ff49a-dddc-4be0-a442-db8e0604afdd",
   "metadata": {},
   "source": [
    "## 3.2. Handle Missing Values Appropriatelylues\r\n",
    "\r\n",
    "While LightGBM can handle missing values natively, preprocessing missing data can still improve model perfor \n",
    "ance:\r\n",
    "\r\n",
    "- **Analyze Missingness**:\r\n",
    "  - Understand Patterns: Determine if data is missing at random or if there's a pattern.\r\n",
    "  - Feature Engineering: Consider creating an indicator variable that flags missing values, allowing the model to learn from the missingness itself.\r\n",
    "\r\n",
    "- **Imputation Techniques**:\r\n",
    "  - Mean/Median Imputation: For numerical features, replace missing values with the mean or median.\r\n",
    "  - Mode Imputation: For categorical features, replace missing values with the mode.\r\n",
    "  - Advanced Imputation: Use algorithms like K-Nearest Neighbors (KNN) or iterative imputation for more sophisticated estimates.\r\n",
    "\r\n",
    "- **Tip**: Always be cautious when imputing to avoid introducing bias. Sometimes, letting LightGBM handle missing values is the best approach."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b96f24-4ff2-4d00-84d7-b66cfddf2d9e",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d739e4-5f6d-437d-b6bf-6faf20eb326f",
   "metadata": {},
   "source": [
    "## 3.3. Encode Categorical Variables Effectively\n",
    "\r\n",
    "LightGBM can handle categorical features directly, but proper encoding can still enhance performance:\r\n",
    "\r\n",
    "- **Categorical Feature Specification**:\r\n",
    "  - When using LightGBM, specify which features are categorical. This allows the model to apply optimal split algorithms for categorical data.\r\n",
    "\r\n",
    "- **Encoding Methods**:\r\n",
    "  - Label Encoding: Assign a unique integer to each category.\r\n",
    "  - One-Hot Encoding: Create binary columns for each category (be cautious with high cardinality features).\r\n",
    "  - Target Encoding: Replace categories with the mean of the target variable for that category (risk of overfitting; use with caution).\r\n",
    "\r\n",
    "- **Tip**: For high-cardinality categorical features, consider combining rare categories or using target encoding with regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e88cdd3-a2ec-41c7-b6fb-3c72010202f5",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2d16595-c9f2-490f-8540-e1d502bbd903",
   "metadata": {},
   "source": [
    "## 3.4. Handle Class Imbalance\r\n",
    "\r\n",
    "In fraud detection, the dataset is often imbalanced, with far fewer fraudulent cases:\r\n",
    "\r\n",
    "- **Resampling Techniques**:\r\n",
    "  - Oversampling: Duplicate or synthesize new instances of the minority class (e.g., SMOTE).\r\n",
    "  - Undersampling: Remove instances from the majority class.\r\n",
    "\r\n",
    "- **Adjust Class Weights**:\r\n",
    "  - Use the scale_pos_weight parameter in LightGBM to balance the impact of positive and negative samples.\r\n",
    "\r\n",
    "- **Evaluation Metrics**:\r\n",
    "  - Use appropriate metrics like Precision, Recall, F1-Score, or Area Under the Precision-Recall Curve (AUPRC) instead of accuracy.\r\n",
    "\r\n",
    "- **Tip**: Always validate the impact of resampling methods using cross-validation to prevent overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8036f58-14db-4c66-8c4b-27eb03037321",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab7cd34-81ed-4664-a16c-39fc7747a299",
   "metadata": {},
   "source": [
    "## 3.5. Outlier Detection and Handling\r\n",
    "\r\n",
    "Outliers can skew the model's understanding of the data distribution:\r\n",
    "\r\n",
    "- **Identify Outliers**:\r\n",
    "  - Use statistical methods (e.g., Z-score, IQR) or visualization techniques (e.g., box plots) to detect outliers.\r\n",
    "\r\n",
    "- **Handle Outliers**:\r\n",
    "  - Removal: Exclude outliers if they are errors or not representative.\r\n",
    "  - Transformation: Apply transformations (e.g., log, square root) to reduce the impact.\r\n",
    "  - Capping/Flooring: Limit the values to a certain range (winsorization).\r\n",
    "\r\n",
    "- **Tip**: In fraud detection, outliers might actually be fraudulent cases. Carefully assess before removing them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0294e52-3eda-466b-991a-b8a36be15a75",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ecb3e7-3900-4f4b-9252-c2c660355492",
   "metadata": {},
   "source": [
    "## 3.6. Address Multicollinearity\r\n",
    "\r\n",
    "Although tree-based models are less sensitive to multicollinearity, reducing highly correlated features can still be beneficial:\r\n",
    "\r\n",
    "- **Detect Multicollinearity**:\r\n",
    "  - Compute correlation matrices to identify highly correlated pairs of features.\r\n",
    "\r\n",
    "- **Feature Selection**:\r\n",
    "  - Remove one of the features from highly correlated pairs.\r\n",
    "\r\n",
    "- **Dimensionality Reduction**:\r\n",
    "  - Use techniques like PCA if appropriate.\r\n",
    "\r\n",
    "- **Tip**: Be cautious, as removing features may also remove valuable information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c32792-9cb1-4d7e-89df-121a66ed6e76",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "356b3837-78a5-4d32-bfff-0738e0cd4250",
   "metadata": {},
   "source": [
    "## 3.7. Feature Importance, Selection and Dimensionality Reduction\r\n",
    "\r\n",
    "### 3.7.1. Feature Selection and Dimensionality Reduction\r\n",
    "\r\n",
    "Reducing the number of features can help the model focus on the most informative ones:\r\n",
    "\r\n",
    "- **Correlation Analysis**: Remove features that are highly correlated with each other to reduce redundancy.\r\n",
    "- **Variance Thresholding**: Eliminate features with little to no variance, as they provide minimal information.\r\n",
    "- **Feature Importance**: Use model-based feature importance scores to identify and remove less important features.\r\n",
    "- **Dimensionality Reduction Techniques**: Techniques like Principal Component Analysis (PCA) can be used, though they may not be as beneficial for tree-based models.\r\n",
    "- **Tip**: Be cautious not to remove features that, while individually weak, may be important in combination wit\n",
    " o# rs.\r\n",
    "\r\n",
    "#### 3.7.2. Utilizing Feature Importance\r\n",
    "\r\n",
    "Understanding which features contribute most to the model can guide further preprocessing:\r\n",
    "\r\n",
    "- **Interpret Feature Importances**: Use LightGBM's built-in feature importance plots.\r\n",
    "- **Refine Features**: Focus on engineering and cleaning the most important features.\r\n",
    "- **Tip**: Features with low importance may be candidates for removal, but consider interactions before making decisions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e23c3459-bc82-49ce-8689-ff1b98b8006d",
   "metadata": {},
   "source": [
    "### 3.7.3. Feature Creation\n",
    "\n",
    "\r\n",
    "Creating new features can help the model capture underlying patterns:\r\n",
    "\r\n",
    "- **Domain Knowledge**:\r\n",
    "  - Use your understanding of the data to create meaningful features. For fraud detection, consider features like transaction frequency, average transaction amount, or time since last transacion.\r\n",
    "\r\n",
    "- **Interactions**:\r\n",
    "  - Create interaction terms between features that may have a combinedeffect.\r\n",
    "\r\n",
    "- **Datetime Features**:\r\n",
    "  - Extract features from timestamps, such as hour of the day, day of the week, or whether a transaction occurred ona weekend.\r\n",
    "\r\n",
    "- **Aggregations**:\r\n",
    "  - Compute aggregated statistics (mean, sum, count) over groups, such as customer ID or prouct category.\r\n",
    "\r\n",
    "- **Tip**: Feature engineering can significantly impact model performance, especially in complex tasks like fraud detection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc43fc78-12ba-458b-892d-28e6433a6131",
   "metadata": {},
   "source": [
    "### 3.7.4. Handle Imbalanced Feature Distribution\r\n",
    "\r\n",
    "If certain features have skewed distributions:\r\n",
    "\r\n",
    "- **Transformation**:\r\n",
    "  - Apply logarithmic or Box-Cox transformations to normalize the distribution.\r\n",
    "\r\n",
    "- **Binning**:\r\n",
    "  - Discretize continuous variables into bins if it makes sense for the data.\r\n",
    "\r\n",
    "- **Tip**: Normalizing skewed features can help the model learn patterns more effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bbc079e-7472-40da-9866-5809fb98a4f8",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b0253d6-1784-43c3-9d08-553c71642cac",
   "metadata": {},
   "source": [
    "## 3.8. Balance Data Across Subsets (If Segmented)\r\n",
    "\r\n",
    "If you decide to segment your data (e.g., by product type):\r\n",
    "\r\n",
    "- **Ensure Sufficient Data**:\r\n",
    "  - Verify that each subset has enough data to train a robust model.\r\n",
    "\r\n",
    "- **Consistent Preprocessing**:\r\n",
    "  - Apply the same preprocessing steps across all subsets to maintain consistency.\r\n",
    "\r\n",
    "- **Tip**: Compare the performance of segmented models versus a unified model to determine the best approach."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02cbb707-72d0-467d-a712-006cc65513de",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca3d7f2-f248-4efa-9f21-76e6613bb857",
   "metadata": {},
   "source": [
    "## 3.9. Monitor and Validate Model Performance\r\n",
    "\r\n",
    "Regularly check how preprocessing affects the model:\r\n",
    "\r\n",
    "- **Use Validation Sets**:\r\n",
    "  - Keep a separate hold-out set to evaluate the impact of preprocessing seps.\r\n",
    "\r\n",
    "- **Avoid Overfitting**:\r\n",
    "  - Monitor for signs of overfitting, such as a large gap between training and validation perfrmance.\r\n",
    "\r\n",
    "- **Performance Metrics**:\r\n",
    "  - Choose appropriate metrics that reflect the business objectives (e.g., minimizing false positives in fraud etection).\r\n",
    "\r\n",
    "- **Tip**: Continuous validation helps ensure that preprocessing steps are beneficial."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ec2e8f-0879-45b6-b485-2db74f7c2f92",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e59525-1eb1-4bea-83c6-a09ae628185a",
   "metadata": {},
   "source": [
    "# 4. Model Development"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec67bfd1-a90e-4b16-a64b-aa9f477162c2",
   "metadata": {},
   "source": [
    "## 4.1. Model Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f90a470c-02e9-4814-a73a-d98dfe1f0a42",
   "metadata": {},
   "source": [
    "## 4.2. Cross-Validation and Hyperparameter Tuning\r\n",
    "\r\n",
    "Optimizing model parameters ensures the model generalizes well:\r\n",
    "\r\n",
    "- **Cross-Validation**:\r\n",
    "  - Use techniques like K-Fold cross-validation to assess model performance reliably.\r\n",
    "\r\n",
    "- **Hyperparameter Optimization**:\r\n",
    "  - Adjust parameters like num_leaves, max_depth, learning_rate, and regularization terms (lambda_l1, lambda_l2).\r\n",
    "\r\n",
    "- **Automated Tuning Tools**:\r\n",
    "  - Use libraries like Optuna, Hyperopt, or GridSearchCV for systematic tuning.\r\n",
    "\r\n",
    "- **Tip**: Careful tuning can significantly improve model performance beyond default settings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f4fb343-cb3c-452d-8879-d53e098895e5",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ed81a4-4172-44ff-9ffa-d923099ec917",
   "metadata": {},
   "source": [
    "# 5. Evaluation Metrics for Classification\r\n",
    "\r\n",
    "## 5.1. Basic Classification Metrics\r\n",
    "\r\n",
    "- **Accuracy**: The proportion of correct predictions (both true positives and true negatives) among the total number of cases examined.\r\n",
    "- **Precision**: The proportion of true positive predictions among all positive predictions.\r\n",
    "- **Recall (Sensitivity)**: The proportion of true positive predictions among all actual positive cases.\r\n",
    "- **F1 Score**: The harmonic mean of precision and recall, providing a single score that balances both metrics.\r\n",
    "- **ROC Score (AUC-ROC)**:\r\n",
    "  - **Definition**: The Receiver Operating Characteristic (ROC) curve plots the True Positive Rate against the False Positive Rate at various threshold settings.\r\n",
    "  - **AUC-ROC**: The Area Under the ROC Curve (AUC-ROC) provides an aggregate measure of performance across all possible classification thresholds.\r\n",
    "  - **Interpretation**: \r\n",
    "    - AUC of 0.5 suggests no discriminative ability (equivalent to random guessing)\r\n",
    "    - AUC of 1.0 indicates perfect discrimination\r\n",
    "  - **Advantages**:\r\n",
    "    - Insensitive to class imbalance\r\n",
    "    - Provides a single measure of classifier performance\r\n",
    "\r\n",
    "## 5.2. Using Probabilities vs. Hard Classifications\r\n",
    "\r\n",
    "In many cases, it's preferable to use probabilities rather than hard classifications:\r\n",
    "\r\n",
    "- **Probabilities**: The model's estimated likelihood of a sample belonging to the positive class.\r\n",
    "- **Hard Classifications**: Binary outcomes (0 or 1) based on a fixed threshold applied to probabilities.\r\n",
    "\r\n",
    "### Advantages of Using Probabilities:\r\n",
    "\r\n",
    "1. **Flexibility in Threshold Setting**: Allows for adjusting the decision threshold based on specific business needs without retraining the model.\r\n",
    "2. **Richer Information**: Provides more nuanced information about the model's confidence in its predictions.\r\n",
    "3. **Better for Ensemble Methods**: Easier to combine predictions from multiple models.\r\n",
    "4. **Improved Metric Calculation**: Metrics like AUC-ROC and log loss use probabilities for more accurate performance assessment.\r\n",
    "\r\n",
    "## 5.3. Other Important Metrics\r\n",
    "\r\n",
    "- **Precision-Recall Curve**: Similar to ROC but focuses on the trade-off between precision and recall.\r\n",
    "- **Log Loss**: Measures the performance of a classification model where the prediction is a probability value between 0 and 1.\r\n",
    "- **Confusion Matrix**: A table layout of prediction results, showing the numbers of true positives, false positives, true negatives, and false negatives.\r\n",
    "\r\n",
    "## 5.4. Choosing the Right Metric\r\n",
    "\r\n",
    "The choice of metric depends on your specific problem and goals:\r\n",
    "\r\n",
    "- **Balanced Dataset**: Accuracy might be sufficient.\r\n",
    "- **Imbalanced Dataset**: Consider precision, recall, F1 score, or AUC-ROC.\r\n",
    "- **Costs of False Positives/Negatives**: Choose metrics that align with the cost structure of errors in your domain.\r\n",
    "\r\n",
    "**Tip**: In fraud detection, where class imbalance is common and the cost of false negatives (missed frauds) might be high, metrics like recall, precision-recall AUC, or custom weighted metrics are often valuable alongside AUC-ROC."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d0ef3d-18f2-465a-b636-4c74a494e252",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7700f5bb-25c5-444e-9953-801122394dff",
   "metadata": {},
   "source": [
    "# 6. Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70df1a28-937a-4ecc-a6da-8aad79b6c830",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
